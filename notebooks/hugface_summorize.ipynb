{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eboraks/miniconda3/envs/icog/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/eboraks/Projects/icognition-backend/notebooks/hugface_summorize.ipynb Cell 1\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/eboraks/Projects/icognition-backend/notebooks/hugface_summorize.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/eboraks/Projects/icognition-backend/notebooks/hugface_summorize.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     TokenClassificationPipeline,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/eboraks/Projects/icognition-backend/notebooks/hugface_summorize.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     AutoModelForTokenClassification,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/eboraks/Projects/icognition-backend/notebooks/hugface_summorize.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     pipeline,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/eboraks/Projects/icognition-backend/notebooks/hugface_summorize.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/eboraks/Projects/icognition-backend/notebooks/hugface_summorize.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipelines\u001b[39;00m \u001b[39mimport\u001b[39;00m AggregationStrategy\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/eboraks/Projects/icognition-backend/notebooks/hugface_summorize.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer, util\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/eboraks/Projects/icognition-backend/notebooks/hugface_summorize.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/eboraks/Projects/icognition-backend/notebooks/hugface_summorize.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TokenClassificationPipeline,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline,\n",
    ")\n",
    "from transformers.pipelines import AggregationStrategy\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "# import pandas as pd\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "st_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = st_model.encode(\"places\")\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pegasus_summarizer = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "\n",
    "pszemraj_summarizer = pipeline(\"summarization\", model=\"pszemraj/led-base-book-summary\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = '/home/eboraks/Projects/icognition_backend/data/icog_pages/bergum.medium.com%2Ffour-mistakes-when-introducing-embeddings-and-vector-search-d39478a568c5.json'\n",
    "with open(path_file, 'r') as f:\n",
    "    page = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs(page) -> list:\n",
    "    for k, v in page['paragraphs'].items():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bergum.medium.com/four-mistakes-when-introducing-embeddings-and-vector-search-d39478a568c5#\n"
     ]
    }
   ],
   "source": [
    "paragaphs = [v for k, v in page['paragraphs'].items()]\n",
    "print(page['url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, token_max_length=512, summary_min_length=80, summary_max_length=120) -> str:\n",
    "    \"\"\"This method summarizes text.\n",
    "\n",
    "    Args:\n",
    "        text (str): text from which the summary should be generated\n",
    "    \"\"\"\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained('google/pegasus-xsum')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "    print(tokenizer.model_max_length)\n",
    "\n",
    "    tokens_input = tokenizer.encode(\n",
    "        \"summarize: \" + text, return_tensors='pt', max_length=token_max_length, truncation=True)\n",
    "    ids = model.generate(\n",
    "        tokens_input, min_length=summary_min_length, max_length=summary_max_length)\n",
    "    summary = tokenizer.decode(ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/pegasus-xsum')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "print(tokenizer.model_max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersTextSummarizer():\n",
    "    def __init__ (self, model_key = 'facebook/bart-large-cnn', language = 'en'):\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_key)\n",
    "\n",
    "        self._language = language\n",
    "\n",
    "        self._model = AutoModelForSeq2SeqLM.from_pretrained(model_key)\n",
    "\n",
    "        self._device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def __chunk_text(self, paragraphs):\n",
    "        \n",
    "        chunks = []\n",
    "\n",
    "        chunk = ''\n",
    "\n",
    "        length = 0\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            tokenized_sentence = self._tokenizer.encode(paragraph, truncation=False, max_length=None, return_tensors='pt') [0]\n",
    "\n",
    "            if len(tokenized_sentence) > self._tokenizer.model_max_length:\n",
    "                continue\n",
    "\n",
    "            length += len(tokenized_sentence)\n",
    "\n",
    "            if length <= self._tokenizer.model_max_length:\n",
    "                chunk = chunk + ' ' + paragraph\n",
    "            else:\n",
    "                chunks.append(chunk.strip())\n",
    "                chunk = paragraph\n",
    "                length = len(tokenized_sentence)\n",
    "\n",
    "        if len(chunk) > 0:\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def __clean_text(self, text):\n",
    "      if text.count('.') == 0:\n",
    "        return text.strip()\n",
    "\n",
    "      end_index = text.rindex('.') + 1\n",
    "\n",
    "      return text[0 : end_index].strip()\n",
    "\n",
    "    def summarize(self, text, beams=5, *args, **kwargs):\n",
    "        chunk_texts = self.__chunk_text(text)\n",
    "\n",
    "        chunk_summaries = []\n",
    "\n",
    "        for chunk_text in chunk_texts:\n",
    "            input_tokenized = self._tokenizer.encode(chunk_text, return_tensors='pt')\n",
    "\n",
    "            input_tokenized = input_tokenized.to(self._device)\n",
    "\n",
    "            summary_ids = self._model.to(self._device).generate(\n",
    "                input_tokenized, \n",
    "                length_penalty=3.0, \n",
    "                min_length = int(0.1 * len(chunk_text)), \n",
    "                max_length = int(0.2 * len(chunk_text)), \n",
    "                early_stopping=True, \n",
    "                num_beams=beams, \n",
    "                no_repeat_ngram_size=2)\n",
    "\n",
    "            output = [self._tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in summary_ids]\n",
    "\n",
    "            chunk_summaries.append(output)\n",
    "\n",
    "        summaries = [ self.__clean_text(text) for chunk_summary in chunk_summaries for text in chunk_summary ]\n",
    "\n",
    "        return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TransformersTextSummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('There has been a surge in interest in how we can represent data in a way '\n",
      " 'that is more useful to computers than the old-fashioned text-based retrieval '\n",
      " 'methods such as Huggingface and query engines. How can we transform data '\n",
      " 'into useful representation using Transformer- based models? how to learn to '\n",
      " 'represent embeddings in text and BERT in search and search engine '\n",
      " 'algorithms.how to train a Transformer based representation learning '\n",
      " 'model.How to transfer learning pipelining to snowboarding, windsurfing, '\n",
      " 'surfing, and other fun activities.')\n",
      "('In our series of posts on Natural Language Processing, we have looked at how '\n",
      " 'the BERT model can be used to generate vector representations of a text '\n",
      " 'chunk, and how to avoid making mistakes in embedding the model into any '\n",
      " 'other NLP model architecture (see How not to use NLP models in your own work '\n",
      " 'for more details on this topic.) Today we are going to look at the '\n",
      " 'Transformer architecture, where we can take some text and tokenize that text '\n",
      " 'into a fixed vocabulary to obtain a set of numeric ids, a mapping between '\n",
      " 'free and hard-coded identifiers, as well as a network output with a maximum '\n",
      " 'of 512 words (input context length limitation) and a dimensionality N, '\n",
      " 'depending on the type of bert-base model we use (Vanilla). The vanilla '\n",
      " 'vanilla Transformer model uses the same architecture as the Word2vec model, '\n",
      " 'but it uses a different vector representation for each word in the input '\n",
      " 'word, so it is not as useful for embedding a chunk of text as it was for '\n",
      " 'Word 2vec, which used cosine vectors to represent a single word; see how we '\n",
      " 'did this in our previous post on embedding.')\n",
      "('The following is a description of some of the ways in which we can train web '\n",
      " 'search models, such as BERT and MS MARCO, and how they can be deployed in '\n",
      " 'the real world, in a variety of use cases, from search engine optimisation '\n",
      " 'to social media management, to web analytics and search marketing, among '\n",
      " 'many other uses, as part of a research project at the Massachusetts '\n",
      " 'Institute of Technology (MIT) and the University of California, Los Angeles '\n",
      " '(UCLA) under the auspices of NSF, the National Science Foundation (NSF), the '\n",
      " 'US Department of Defense (DoD), and other federal, state and local '\n",
      " 'governments, universities and research institutions, including the MIT Media '\n",
      " 'Lab, Harvard University, MIT Technology Review, The New York Times, Google, '\n",
      " 'Microsoft, Twitter, Facebook, Yahoo, LinkedIn, Wikipedia, Reddit, YouTube, '\n",
      " 'NationMaster NationMaster.com, a website of NationMaster, an international '\n",
      " 'NationMaster website, which is run by the International Organization for '\n",
      " 'Migration (IOM), a global network of more than one million member states, '\n",
      " 'with a combined membership of nearly 500 million people, many of whom live '\n",
      " 'and work in developing countries, according to the IOM.')\n",
      "('In our last post, we looked at the accuracy error tolerance of ANN '\n",
      " 'algorithms, and how that compares with other techniques likesupervised '\n",
      " 'learning and natural language processing (NLP) and machine learning (ML) '\n",
      " 'which are much more accurate but have a much higher error rate and can be '\n",
      " 'more expensive to implement and to use in real-time operations such as image '\n",
      " 'search and object recognition.) Here we will look at some of the differences '\n",
      " 'between these two techniques, as well as how they might be used in different '\n",
      " 'use-case situations, so that we can get a better understanding of what we '\n",
      " 'need to do in order to get the most out of our vector search algorithms and '\n",
      " 'the best way to make them cost-effective and efficient in the real world, '\n",
      " 'with the goal of making them more useful in applications where they are used '\n",
      " 'as part of a larger search engine, like image searching or '\n",
      " 'object-recognition applications, or where we are building a retina image '\n",
      " 'scan app, where the user can access the building via a web browser or mobile '\n",
      " 'device, by using an app built on top of an image engine like Google’s Image '\n",
      " 'Search or Facebook’ s Graph Search, among many other popular search engines '\n",
      " 'and artificial intelligence (AI) tools.')\n",
      "('In my last post on ann-benchmark, I looked at the performance and recall of '\n",
      " 'some of the most popular ANN algorithms, and today I’m going to look at '\n",
      " 'performance versus recall in a vector search dataset, as well as the cost of '\n",
      " 'indexing and how it compares to performance on a server with multiple CPU '\n",
      " 'cores (see also my previous post, here: '\n",
      " 'https://www.stackoverflow.com/questions/why-ann-search-performance-is-not-as-good- '\n",
      " 'as-it-should-be-on-a-computer-with-multiple-CPU-cores-for-more-than-one-processors, '\n",
      " 'for more on that).')\n"
     ]
    }
   ],
   "source": [
    "summaries = ts.summarize(text= paragaphs)\n",
    "\n",
    "for  summary in summaries:\n",
    "    pp.pprint(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts2 = TransformersTextSummarizer(model_key='facebook/bart-large-cnn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Embedding-based retrieval (EBR) using vector search is more popular than '\n",
      " 'ever. The answer is better model architectures (e.g., Transformer '\n",
      " 'architecture) and self-supervised representation learning. How useful the '\n",
      " 'representation is, depends on how we learn this transformation and how the '\n",
      " 'learned way to represent data generalizes to new data. Using the direct '\n",
      " 'vector representations from the model that have only been pre-trained will '\n",
      " 'not produce a useful embedding representation for any task. Encoding free '\n",
      " 'text queries and documents. and expecting that the cosine similarity between '\n",
      " 'the two representations can rank the documents by relevance is naive, and '\n",
      " 'the results of that approach give you next to random ranking results. Your '\n",
      " 'learned skills do not transfer to playing golf or swimming. We can train a '\n",
      " 'model using piles of data without human supervision (labeling). Then, once '\n",
      " 'that is done, we can fine-tune the models for tasks where the finetuning '\n",
      " 'requires less labeled data than if we started from scratch. This type of '\n",
      " 'learning pipelining is called transfer learning and can be used to learn to '\n",
      " 'snowboard, windsurfing, surfing, or other fun activities. To obtain a better '\n",
      " 'representation (better than random), we need to tune the weights. For '\n",
      " 'example, average pooling will average the 512 output vectors into a single '\n",
      " 'vector representation of a text chunk, which leads to mistake number 1. See '\n",
      " 'details in How not to use BERT for search ranking for such a task; see how '\n",
      " 'to get the best results in this blog post. Back to the page you came from. '\n",
      " 'Click here to read the full article and to share your thoughts on the topic '\n",
      " 'with the rest of the team. You can send us your feedback by emailing '\n",
      " \"editorial@dailymail.co.uk and we'll post it on our Facebook page and Twitter \"\n",
      " 'account. If you have any questions about the blog, please send them to '\n",
      " 'jennifer.glanfield@mailonline.com. In the meantime, let us know what you '\n",
      " 'think about our blog and our plans for the future of machine learning in the '\n",
      " 'coming months. Thanks for your support and stay tuned for our next issue of '\n",
      " 'The Daily Mail’s ‘Machine Learning in a New and Improved way\\u2009. back to '\n",
      " 'MailOnline home.Back tothe page where you Came From.')\n",
      "('This fine-tuning creates useful embedding representations based on BERT and '\n",
      " 'outcompetes traditional keyword search methods with no learnable parameters, '\n",
      " 'such as BM25, by a very large margin on the MS MARCO dataset. The problem is '\n",
      " 'that when we take a single vector representation model that does not beat BM '\n",
      " '25 in a different domain with slightly different types of documents and '\n",
      " 'questions, many approaches perform poorly. An exact search for neighbors '\n",
      " 'will brute-force calculate the distance between the query and all eligible '\n",
      " 'documents, and the returned k documents are the true nearest neighbors. For '\n",
      " 'example, brute force searching 1M vectors with 128 dimensions takes about '\n",
      " '100ms single-threaded. We can parallelize the search; by using four threads, '\n",
      " 'we can get it down to 25 ms until memory bandwidth hits. If we page the '\n",
      " 'vector data randomly from the disk, it will be slower but still '\n",
      " 'parallelizable. Here there are also many tradeoffs, like disk usage and '\n",
      " 'memory usage. How well the algorithm can be used with real-time CRUD '\n",
      " 'operations. One source of ANN algorithm understanding is '\n",
      " 'https://github.com/erikbern/ann-benchmarks, where different algorithms and '\n",
      " 'algorithms are compared on various vector datasets. This graph displays '\n",
      " 'where implementations are where the recall is 1.@10 (same overlap@ 10) '\n",
      " 'versus the 1 million queries per second (QPS) The graph is for the SIFT '\n",
      " 'dataset where where Recall@1. @10 means that if the QPS algorithm is at '\n",
      " '10²PS, which means a 10³ latency of 1ms and so forth, so fast. These are '\n",
      " 'pretty damn fast algorithms. But renting servers to keep the latency in '\n",
      " 'check can become costly with billions of embeddings. Add high query '\n",
      " 'throughput to the mix and we have a real cost problem. Going down the '\n",
      " 'approximate vector search route, you need an algorithm that can index the '\n",
      " 'data so that searches are less costly than exhaustive searches at the cost '\n",
      " 'of resource usage, indexing processing and processing. In academic research '\n",
      " 'on ANN algorithms, there is a distinct differentiation between high-recall '\n",
      " 'and low-Recall settings. On the other hand, if you are building an image '\n",
      " 'search service with over a billion photo vectors,you don’t necessarily need '\n",
      " 'perfect recall. There are many equally great cat photos and bringing back '\n",
      " 'the exact best cats as deemed most relevant by the model might not be that '\n",
      " 'important.')\n",
      "('If we deploy these algorithms on a server with multiple CPU cores, we can '\n",
      " 'enjoy even more QPS. Not all ANN algorithms give us equally good recall. '\n",
      " 'Algorithms that are up and to the right give the best tradeoff between '\n",
      " 'performance and accuracy, and the lower left quadrant is worse. Note that '\n",
      " 'ann-benchmark can only use open-source algorithms to reproduce on the same '\n",
      " 'runtime. Some commercial and proprietary vector search vendors have unknown '\n",
      " 'recall versus performance tradeoffs. If you hated this post, you could shout '\n",
      " 'out to me over at Twitter https://twitter.com/jobergum.')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summaries = ts2.summarize(paragaphs)\n",
    "for summ in summaries:\n",
    "    pp.pprint(summ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but you input_length is only 803. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=401)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representing unstructured data as embedding vectors and embedding-based retrieval (EBR) using vector search is more popular than ever. The answer is better model architectures, transformer architecture, and self-supervised representation learning. Machine Learning uses machine learning to learn from big data in order to apply machine learning techniques to new data. In this post, we'll focus on text models and BERT models specifically. These are deep neural network models that can be used to rank documents without human supervision. For example, snowboarders do well when they're ranked according to relevance; for golfers, however, they do poorly when it's ranked by relevance. We also briefly delve into the use case for KNN, a distributed clustering framework designed to track how many different types of information each dataset contains.\n",
      "Let us expand on the accuracy error tolerance and why that is use-case dependent. If you're building an image search service with over a billion photo vectors, you don’t necessarily need perfect recall. There are many great cat photos out there, but bringing back the exact best cats as deemed most relevant by the model might not be that important. In academic research on ANN algorithms, there is a distinct differentiation between these extremes, high-recall and low-reall settings. An exhaustive search might be all you needThe exact search for neighbors will brute-force calculate the distance between the query and all eligible documents, and the returned k documents are the true nearest neighbors. The search can be parallelized, multi-threaded, and in many cases, can use optimized HW instructions; vectors are the machine's language. The cost of resource usage and indexing processing may be one of the biggest tradeoffs. However, if we store the vectors in an engine with query engine filtering capabilities, we have a good chance of getting the same result. Using vector datasets, Erik Bjornson compares different ANN algorithms and implementations to compare them. He notes that some commercial and proprietary vector search vendors have unknown \"quality versus performance tradeoffs\" .\n"
     ]
    }
   ],
   "source": [
    "sentences = list(data['paragraphs'].values())\n",
    "chunks = concatenate_sentences_to_chunks(sentences)\n",
    "\n",
    "for chunk in chunks:\n",
    "    summary = pszemraj_summarizer(chunk)\n",
    "    print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_sentences_to_chunks(sentences):\n",
    "  \n",
    "  chunks = []\n",
    "  current_chunk = ''\n",
    "  for sentence in sentences:\n",
    "    current_chunk += sentence.replace('\\n', ' ')\n",
    "    if len(current_chunk.split(' ')) >= 1000:\n",
    "      chunks.append(current_chunk)\n",
    "      current_chunk = ''\n",
    "\n",
    "  chunks.append(current_chunk)\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "st_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keyphrase extraction pipeline\n",
    "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )     \n",
    "\n",
    "\n",
    "    def postprocess(self, all_outputs):\n",
    "        mid_results = super().postprocess(\n",
    "            all_outputs=all_outputs,\n",
    "            aggregation_strategy = AggregationStrategy.AVERAGE,\n",
    "        )\n",
    "        strings = set()\n",
    "        results = []\n",
    "        for kp in mid_results:\n",
    "            \n",
    "            if len(kp.get(\"word\")) < 2:\n",
    "                continue\n",
    "            \n",
    "            if kp.get(\"word\") not in strings:\n",
    "                strings.add(kp.get(\"word\"))\n",
    "                results.append(kp)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(kp_objects, st_model):\n",
    "    keyphrases = [kp.get(\"word\") for kp in kp_objects]\n",
    "    embeddings = st_model.encode(keyphrases)\n",
    "\n",
    "    #Compute cosine similarity between all pairs\n",
    "    cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "    #Add all pairs to a list with their cosine similarity score\n",
    "    all_sentence_combinations = []\n",
    "    for i in range(len(cos_sim)-1):\n",
    "        for j in range(i+1, len(cos_sim)):\n",
    "            all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "    #Sort list by the highest cosine similarity score\n",
    "    all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    print(\"Top-5 most similar pairs:\")\n",
    "    for score, i, j in all_sentence_combinations:\n",
    "        print(\"{} \\t {} \\t {:.4f}\".format(keyphrases[i], keyphrases[j], cos_sim[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ml6team/keyphrase-extraction-distilbert-inspec\"\n",
    "extractor = KeyphraseExtractionPipeline(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'KEY', 'score': 0.82937986, 'word': 'unstructured data', 'start': 13, 'end': 30}\n",
      "{'entity_group': 'KEY', 'score': 0.8330004, 'word': 'embedding vectors', 'start': 34, 'end': 51}\n",
      "{'entity_group': 'KEY', 'score': 0.995934, 'word': 'vector search', 'start': 94, 'end': 107}\n",
      "{'entity_group': 'KEY', 'score': 0.74980867, 'word': 'embeddings', 'start': 220, 'end': 230}\n",
      "{'entity_group': 'KEY', 'score': 0.3863214, 'word': 'ebr', 'start': 315, 'end': 318}\n",
      "{'entity_group': 'KEY', 'score': 0.83149266, 'word': 'roy keyes', 'start': 395, 'end': 404}\n",
      "{'entity_group': 'KEY', 'score': 0.53966975, 'word': 'embedding', 'start': 562, 'end': 571}\n",
      "{'entity_group': 'KEY', 'score': 0.42835072, 'word': 'vectors', 'start': 572, 'end': 579}\n",
      "{'entity_group': 'KEY', 'score': 0.9178834, 'word': 'transformations', 'start': 783, 'end': 798}\n",
      "{'entity_group': 'KEY', 'score': 0.99932146, 'word': 'academia', 'start': 827, 'end': 835}\n",
      "{'entity_group': 'KEY', 'score': 0.9998516, 'word': 'representation learning', 'start': 862, 'end': 885}\n",
      "{'entity_group': 'KEY', 'score': 0.8128295, 'word': 'bidirectional encoder representations', 'start': 1071, 'end': 1108}\n",
      "{'entity_group': 'KEY', 'score': 0.8067933, 'word': 'language', 'start': 1580, 'end': 1588}\n",
      "{'entity_group': 'KEY', 'score': 0.99991655, 'word': 'machine learning', 'start': 1905, 'end': 1921}\n",
      "{'entity_group': 'KEY', 'score': 0.99486095, 'word': 'search', 'start': 2111, 'end': 2117}\n",
      "{'entity_group': 'KEY', 'score': 0.501642, 'word': 'keyes', 'start': 2177, 'end': 2182}\n",
      "{'entity_group': 'KEY', 'score': 0.9250195, 'word': 'learning', 'start': 2359, 'end': 2367}\n"
     ]
    }
   ],
   "source": [
    "kps = extractor(data['full_text'])\n",
    "for kp in kps:\n",
    "    print(kp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb = lancedb.connect(\"/home/eboraks/Projects/icognition_backend/data/lance.db\")\n",
    "ltable = ldb.open_table(\"ldb_from_joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ldb_from_joined']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldb.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123759</td>\n",
       "      <td>Organization of American States</td>\n",
       "      <td>[0.00659605, -0.0870092, -0.020704046, 0.05541...</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35657</td>\n",
       "      <td>state of the United States</td>\n",
       "      <td>[0.0022839634, 0.022262672, 0.010826143, 0.056...</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>[0.0574986, -0.013809884, 0.009366523, 0.02882...</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578170</td>\n",
       "      <td>contiguous United States</td>\n",
       "      <td>[0.062691376, -0.017657915, -0.03934625, 0.017...</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166433</td>\n",
       "      <td>The United States of America</td>\n",
       "      <td>[0.04860917, -0.011422557, -0.0030388979, 0.02...</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>258664</td>\n",
       "      <td>United States of Europe</td>\n",
       "      <td>[0.07275655, -0.0016237864, -0.04007106, -0.00...</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>490417</td>\n",
       "      <td>Andean states</td>\n",
       "      <td>[0.007954296, 0.024982398, -0.028406274, 0.070...</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>272160</td>\n",
       "      <td>geography of the United States</td>\n",
       "      <td>[0.13028723, -0.057744168, 0.030923432, 0.0405...</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>304051</td>\n",
       "      <td>United States in the 1950s</td>\n",
       "      <td>[0.017661104, 0.04208232, -0.052255336, 0.0628...</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>529878</td>\n",
       "      <td>50 State Quarters</td>\n",
       "      <td>[0.0041106795, 0.06610076, -0.027842931, 0.020...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                             text  \\\n",
       "0  123759  Organization of American States   \n",
       "1   35657       state of the United States   \n",
       "2      30         United States of America   \n",
       "3  578170         contiguous United States   \n",
       "4  166433     The United States of America   \n",
       "5  258664          United States of Europe   \n",
       "6  490417                    Andean states   \n",
       "7  272160   geography of the United States   \n",
       "8  304051       United States in the 1950s   \n",
       "9  529878                50 State Quarters   \n",
       "\n",
       "                                              vector  score  \n",
       "0  [0.00659605, -0.0870092, -0.020704046, 0.05541...   0.80  \n",
       "1  [0.0022839634, 0.022262672, 0.010826143, 0.056...   0.88  \n",
       "2  [0.0574986, -0.013809884, 0.009366523, 0.02882...   0.93  \n",
       "3  [0.062691376, -0.017657915, -0.03934625, 0.017...   0.94  \n",
       "4  [0.04860917, -0.011422557, -0.0030388979, 0.02...   0.94  \n",
       "5  [0.07275655, -0.0016237864, -0.04007106, -0.00...   0.96  \n",
       "6  [0.007954296, 0.024982398, -0.028406274, 0.070...   0.97  \n",
       "7  [0.13028723, -0.057744168, 0.030923432, 0.0405...   0.97  \n",
       "8  [0.017661104, 0.04208232, -0.052255336, 0.0628...   0.98  \n",
       "9  [0.0041106795, 0.06610076, -0.027842931, 0.020...   1.00  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = ltable.search(st_model.encode(\"America, is a country comprising 50 states\")).limit(10).to_df()\n",
    "query['score'] = query['score'].round(2)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4227]])\n",
      "tensor([[0.3723]])\n"
     ]
    }
   ],
   "source": [
    "aa = st_model.encode(\"usa\")\n",
    "bb = st_model.encode(\"usa america\")\n",
    "dd = st_model.encode(\"United States of America federal republic in North America\")\n",
    "print(util.cos_sim(bb, dd))\n",
    "cc = st_model.encode(\"Usa city in Ōita Prefecture, Japan\")\n",
    "print(util.cos_sim(bb, cc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9996"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/eboraks/Projects/icognition_backend/data/embeddings/embeddings_0_10000.csv\")\n",
    "\n",
    "#df1 = df[df['vector'] != \"['EMBEDDING_NOT_GENERATED']\"]\n",
    "len(df[df['text'].notnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_cleaner\n",
    "from spacy_cleaner.processing import removers, replacers, mutators\n",
    "\n",
    "model = spacy.load(\"en_core_web_sm\")\n",
    "pipeline = spacy_cleaner.Pipeline(\n",
    "    model,\n",
    "    removers.remove_stopword_token,\n",
    "    removers.remove_punctuation_token,\n",
    "    mutators.mutate_lemma_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Progress: 100%|██████████| 1/1 [00:00<00:00, 80.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['United States America USA commonly know']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = [\"United States of America (USA), commonly known as the\"]\n",
    "\n",
    "pipeline.clean(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "dbpath = '/home/eboraks/Projects/icognition_backend/data/wikidata_2.db'\n",
    "connection = sqlite3.connect(dbpath)\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT vector \n",
    "        FROM joined\n",
    "        WHERE item_id = 980509\n",
    "        \"\"\"\n",
    "        \n",
    "vector = []\n",
    "for row in cursor.execute(query):\n",
    "    vector = json.loads(row[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltable.create_fts_index(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m ltable\u001b[39m.\u001b[39;49msearch(\u001b[39m\"\u001b[39;49m\u001b[39membedding\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mto_df()\n",
      "File \u001b[0;32m~/anaconda3/envs/hugface/lib/python3.9/site-packages/lancedb/query.py:166\u001b[0m, in \u001b[0;36mLanceFtsQueryBuilder.to_df\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m index \u001b[39m=\u001b[39m tantivy\u001b[39m.\u001b[39mIndex\u001b[39m.\u001b[39mopen(index_path)\n\u001b[1;32m    165\u001b[0m \u001b[39m# get the scores and doc ids\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m row_ids, scores \u001b[39m=\u001b[39m search_index(index, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_query, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_limit)\n\u001b[1;32m    167\u001b[0m scores \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39marray(scores)\n\u001b[1;32m    168\u001b[0m output_tbl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_table\u001b[39m.\u001b[39mto_lance()\u001b[39m.\u001b[39mtake(row_ids, columns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_columns)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "df = ltable.search(\"embedding\").to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 most similar pairs:\n",
      "embeddings \t embedding \t 0.9631\n",
      "embedding vectors \t embedding \t 0.8369\n",
      "embedding vectors \t embeddings \t 0.8364\n",
      "vector search \t vectors \t 0.6999\n",
      "roy keyes \t keyes \t 0.6559\n",
      "embedding vectors \t vectors \t 0.6474\n",
      "machine learning \t learning \t 0.6312\n",
      "representation learning \t bidirectional encoder representations \t 0.5882\n",
      "vector search \t search \t 0.5352\n",
      "representation learning \t learning \t 0.5199\n",
      "embeddings \t bidirectional encoder representations \t 0.5172\n",
      "embedding \t representation learning \t 0.5050\n",
      "language \t learning \t 0.5043\n",
      "embedding vectors \t bidirectional encoder representations \t 0.5009\n",
      "embeddings \t representation learning \t 0.4993\n",
      "embedding vectors \t vector search \t 0.4958\n",
      "embedding \t bidirectional encoder representations \t 0.4934\n",
      "embedding vectors \t representation learning \t 0.4900\n",
      "representation learning \t machine learning \t 0.4730\n",
      "machine learning \t search \t 0.4443\n",
      "academia \t learning \t 0.4350\n",
      "vector search \t machine learning \t 0.4188\n",
      "search \t learning \t 0.4099\n",
      "academia \t language \t 0.3913\n",
      "embedding \t learning \t 0.3876\n",
      "language \t machine learning \t 0.3609\n",
      "vectors \t transformations \t 0.3569\n",
      "transformations \t learning \t 0.3567\n",
      "unstructured data \t machine learning \t 0.3434\n",
      "embeddings \t learning \t 0.3387\n",
      "embedding \t vectors \t 0.3374\n",
      "embeddings \t vectors \t 0.3368\n",
      "embedding \t machine learning \t 0.3351\n",
      "search \t keyes \t 0.3301\n",
      "unstructured data \t vector search \t 0.3239\n",
      "embeddings \t machine learning \t 0.3141\n",
      "unstructured data \t search \t 0.3042\n",
      "vector search \t learning \t 0.2987\n",
      "language \t search \t 0.2984\n",
      "vectors \t machine learning \t 0.2946\n",
      "transformations \t representation learning \t 0.2865\n",
      "academia \t search \t 0.2852\n",
      "academia \t machine learning \t 0.2828\n",
      "vector search \t embeddings \t 0.2795\n",
      "vectors \t representation learning \t 0.2786\n",
      "keyes \t learning \t 0.2784\n",
      "representation learning \t language \t 0.2783\n",
      "language \t keyes \t 0.2746\n",
      "bidirectional encoder representations \t learning \t 0.2741\n",
      "vector search \t embedding \t 0.2735\n",
      "vectors \t learning \t 0.2725\n",
      "machine learning \t keyes \t 0.2723\n",
      "vector search \t representation learning \t 0.2714\n",
      "embedding \t language \t 0.2676\n",
      "unstructured data \t embeddings \t 0.2674\n",
      "transformations \t keyes \t 0.2664\n",
      "embedding vectors \t learning \t 0.2652\n",
      "transformations \t machine learning \t 0.2634\n",
      "transformations \t language \t 0.2601\n",
      "embeddings \t search \t 0.2537\n",
      ") \t keyes \t 0.2536\n",
      "embeddings \t language \t 0.2499\n",
      "ebr \t search \t 0.2493\n",
      "unstructured data \t embedding \t 0.2473\n",
      "embedding vectors \t machine learning \t 0.2460\n",
      "transformations \t bidirectional encoder representations \t 0.2450\n",
      "embedding \t search \t 0.2418\n",
      "embedding \t transformations \t 0.2415\n",
      "vector search \t transformations \t 0.2387\n",
      "ebr \t academia \t 0.2384\n",
      "ebr \t learning \t 0.2372\n",
      "bidirectional encoder representations \t language \t 0.2334\n",
      "vectors \t bidirectional encoder representations \t 0.2292\n",
      "bidirectional encoder representations \t machine learning \t 0.2264\n",
      "embedding \t academia \t 0.2262\n",
      "embeddings \t transformations \t 0.2215\n",
      "vectors \t language \t 0.2193\n",
      "roy keyes \t search \t 0.2110\n",
      "unstructured data \t representation learning \t 0.2101\n",
      "ebr \t language \t 0.2070\n",
      "embedding vectors \t transformations \t 0.2067\n",
      "representation learning \t search \t 0.2052\n",
      "vectors \t search \t 0.2048\n",
      "vectors \t keyes \t 0.2036\n",
      "unstructured data \t bidirectional encoder representations \t 0.2032\n",
      "embeddings \t academia \t 0.2030\n",
      "unstructured data \t embedding vectors \t 0.2009\n",
      "ebr \t roy keyes \t 0.2005\n",
      ") \t language \t 0.1963\n",
      "transformations \t ) \t 0.1952\n",
      "ebr \t embedding \t 0.1934\n",
      "embeddings \t keyes \t 0.1929\n",
      "unstructured data \t learning \t 0.1909\n",
      "embedding vectors \t ebr \t 0.1882\n",
      "roy keyes \t learning \t 0.1871\n",
      "vector search \t keyes \t 0.1855\n",
      "academia \t keyes \t 0.1835\n",
      "transformations \t search \t 0.1827\n",
      "vector search \t bidirectional encoder representations \t 0.1825\n",
      "roy keyes \t academia \t 0.1820\n",
      "ebr \t keyes \t 0.1791\n",
      "ebr \t vectors \t 0.1779\n",
      "bidirectional encoder representations \t keyes \t 0.1743\n",
      "embedding \t keyes \t 0.1697\n",
      "academia \t ) \t 0.1693\n",
      ") \t learning \t 0.1683\n",
      "roy keyes \t machine learning \t 0.1677\n",
      "roy keyes \t transformations \t 0.1674\n",
      "embedding vectors \t language \t 0.1674\n",
      "embedding vectors \t search \t 0.1664\n",
      "embeddings \t ebr \t 0.1618\n",
      "unstructured data \t keyes \t 0.1609\n",
      "unstructured data \t language \t 0.1563\n",
      "vectors \t ) \t 0.1553\n",
      "transformations \t academia \t 0.1546\n",
      "ebr \t ) \t 0.1504\n",
      "embedding vectors \t keyes \t 0.1464\n",
      "vector search \t ebr \t 0.1462\n",
      "ebr \t machine learning \t 0.1445\n",
      ") \t search \t 0.1428\n",
      "roy keyes \t language \t 0.1382\n",
      "unstructured data \t vectors \t 0.1341\n",
      "bidirectional encoder representations \t search \t 0.1330\n",
      "unstructured data \t ebr \t 0.1322\n",
      "unstructured data \t transformations \t 0.1280\n",
      "representation learning \t keyes \t 0.1260\n",
      "roy keyes \t ) \t 0.1180\n",
      "academia \t representation learning \t 0.1167\n",
      "embedding \t ) \t 0.1135\n",
      "vectors \t academia \t 0.1125\n",
      "embedding vectors \t academia \t 0.1098\n",
      "ebr \t transformations \t 0.1071\n",
      "vector search \t language \t 0.1070\n",
      "vector search \t ) \t 0.0997\n",
      "bidirectional encoder representations \t ) \t 0.0976\n",
      "ebr \t representation learning \t 0.0957\n",
      "unstructured data \t roy keyes \t 0.0953\n",
      "unstructured data \t academia \t 0.0941\n",
      ") \t machine learning \t 0.0927\n",
      "embeddings \t ) \t 0.0884\n",
      "roy keyes \t representation learning \t 0.0872\n",
      "vector search \t roy keyes \t 0.0798\n",
      "embedding vectors \t ) \t 0.0790\n",
      "roy keyes \t bidirectional encoder representations \t 0.0723\n",
      "vector search \t academia \t 0.0716\n",
      "roy keyes \t embedding \t 0.0673\n",
      "embeddings \t roy keyes \t 0.0658\n",
      "ebr \t bidirectional encoder representations \t 0.0571\n",
      "academia \t bidirectional encoder representations \t 0.0551\n",
      "representation learning \t ) \t 0.0506\n",
      "roy keyes \t vectors \t 0.0297\n",
      "embedding vectors \t roy keyes \t 0.0130\n",
      "unstructured data \t ) \t -0.0016\n"
     ]
    }
   ],
   "source": [
    "prune(kps, st_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.777390566811793"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = nlp('machine learning')\n",
    "l = nlp('representation learning')\n",
    "ml.similarity(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity: tensor([[0.6312]])\n"
     ]
    }
   ],
   "source": [
    "#Sentences are encoded by calling model.encode()\n",
    "emb1 = model.encode('machine learning')\n",
    "emb2 = model.encode('learning')\n",
    "\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
